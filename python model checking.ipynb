{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./ML_stuff/jeff_model') #./Milestone_1/trained_model_fold4.h5\n",
    "\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('./ML_stuff/input.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 300)\n",
      "(500, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = data[np.random.choice(data.shape[0], 500, replace=False)]\n",
    "X = data.transpose()[:-1].transpose()\n",
    "\n",
    "values = data.transpose()[-1].astype(int)\n",
    "n_values = np.max(values) + 1\n",
    "y = np.eye(n_values)[values]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "prediction = model.predict(X)\n",
    "\n",
    "m = tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None)\n",
    "m.update_state(prediction, y)\n",
    "print(m.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_123 (Dense)            (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 42,953\n",
      "Trainable params: 42,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[ 0.00786078  0.24179345  0.16351333 -0.01547752 -0.00814214 -0.07526322\n",
      " -0.01191895 -0.00955294 -0.00895992 -0.07435301 -0.0081785   0.05181692\n",
      " -0.16312914 -0.05210665 -0.00510634  0.32351705 -0.16133392 -0.16247587\n",
      "  0.21837536 -0.1490463  -0.00513944  0.14626183  0.04790089  0.07702033\n",
      "  0.01191748 -0.00769351 -0.10057472  0.08642098 -0.18976866  0.05589047\n",
      "  0.35513613  0.          0.06445313 -0.01427482  0.0225263   0.04195891\n",
      " -0.06868553 -0.3609161  -0.0745666  -0.00787111 -0.00720704  0.0372144\n",
      " -0.00534081  0.4343144   0.00718731 -0.01049516 -0.07022673 -0.14008915\n",
      " -0.32992342  0.14564405  0.10184323  0.09588414 -0.05615019  0.34326196\n",
      " -0.20417777 -0.22353597 -0.01207697 -0.34918454  0.09940895 -0.22837703\n",
      "  0.0551411  -0.00774984  0.05016639  0.2461649   0.14815198 -0.0117985\n",
      " -0.30794573 -0.13689919  0.2529445  -0.21315615  0.24436754 -0.09297011\n",
      " -0.01281488  0.11935291 -0.23238938 -0.2453928  -0.01189825 -0.00448751\n",
      " -0.13642418 -0.13912983  0.17554983  0.42713425  0.16583373 -0.00523767\n",
      " -0.12260446 -0.01653939 -0.18610807  0.231478   -0.05033381  0.0902949\n",
      " -0.01209495 -0.04583764 -0.00599312 -0.21574545  0.29938737  0.3379767\n",
      " -0.38473627 -0.00988084  0.19231988 -0.06746837  0.14843744  0.35454354\n",
      " -0.0070108   0.10873178  0.08667756  0.05148249  0.29767674 -0.09425179\n",
      "  0.4794883   0.09750853  0.01742937  0.07968779 -0.12047631 -0.01285768\n",
      "  0.12460902 -0.07985111  0.08689084  0.26290143  0.13970855 -0.01058685\n",
      " -0.02820718  0.02225575 -0.20302552  0.19756053  0.32640105 -0.00676242\n",
      "  0.13246317  0.0421976 ]\n",
      "[[ 0.02931486  0.00404608  0.09069352 ... -0.05927223  0.15939552\n",
      "  -0.10321958]\n",
      " [ 0.00471564  0.07066879  0.17939012 ...  0.08810523  0.25512892\n",
      "   0.14644499]\n",
      " [ 0.03129582 -0.059128   -0.037919   ...  0.0324269  -0.02746029\n",
      "   0.33755064]\n",
      " ...\n",
      " [ 0.39780474 -0.69708246 -0.01103515 ... -0.11292373 -0.01889931\n",
      "  -0.7614661 ]\n",
      " [-0.2782801   0.02658891  0.15731777 ...  0.09546787  0.04314428\n",
      "  -0.3094241 ]\n",
      " [-0.7058203   0.25991425 -0.42944673 ... -0.07997408 -0.64572346\n",
      "   0.27194753]]\n",
      "[ 1.2387977e-01  6.8567544e-02 -4.2655207e-02  2.3684517e-01\n",
      " -3.1508917e-01 -1.9964044e-01 -1.7408943e-01 -3.2356240e-02\n",
      "  1.2569052e-01 -1.0409529e-01 -5.2203313e-03  1.9020829e-01\n",
      " -4.9718898e-02 -9.0678528e-02  5.4738794e-02  7.7671597e-05\n",
      "  9.6028976e-02  2.9378182e-01 -4.0048417e-02 -2.0367266e-01\n",
      "  1.4435373e-01  6.5856292e-03 -1.4662383e-01 -1.4588040e-01\n",
      " -1.5899217e-03  1.3325206e-02  2.1343923e-01 -8.0696279e-03\n",
      " -8.7675259e-02  2.4953437e-01  2.8333536e-01  2.4242248e-01]\n",
      "[[ 1.7664593   1.5590577   0.01457752 ... -1.1818662  -1.0572596\n",
      "   0.60633194]\n",
      " [-0.05243076 -0.2420102   0.13154009 ... -0.06702804  0.06095293\n",
      "   0.28024253]\n",
      " [-0.00375454  0.11205462  0.06979065 ...  0.05296161  0.10098518\n",
      "  -0.13912927]\n",
      " ...\n",
      " [-0.00212909 -0.04914269 -0.01139835 ... -0.02956576  0.05651639\n",
      "   0.087129  ]\n",
      " [-0.03108146 -0.06984004 -0.10968786 ... -0.04302944  0.05915094\n",
      "  -0.44680718]\n",
      " [-0.07806731 -1.7645367  -0.01437744 ...  0.1659154  -0.5058134\n",
      "   0.2690293 ]]\n",
      "[ 0.07656226 -0.00725463 -0.19003026  0.17245886 -0.24026428  0.1317114\n",
      "  0.03247272 -0.18988867  0.21095891]\n",
      "[[ 0.5336367   0.5681504   0.40047267 -0.76906526  0.22639146  0.9474939\n",
      "  -1.0377322  -0.17408223 -1.0611616 ]\n",
      " [ 0.5814638   0.22024745  0.27579337 -0.61808336  0.23873624  0.13830876\n",
      "  -0.01436269 -0.2490862  -0.9147179 ]\n",
      " [ 0.31705484 -0.2860802  -0.30653706 -0.25728875  0.26517126  0.05776853\n",
      "   0.20644559  0.25488546 -0.1251707 ]\n",
      " [ 0.20547841  0.7428122   0.2868372  -1.1965604  -0.42824474 -0.2729309\n",
      "  -0.1328588  -0.01856024  1.38099   ]\n",
      " [ 0.26676083  0.13684115 -0.9517201  -0.9238331   0.67552936 -1.1396706\n",
      "  -0.12332198 -0.4706044   0.12609081]\n",
      " [-0.67531645  0.18090163  0.64618397 -0.23500274  0.33859506 -0.31947127\n",
      "   0.39360866 -0.08948179 -0.2712495 ]\n",
      " [ 0.2707222  -0.10171901 -0.07030194 -0.6964692   0.63906866 -0.3978022\n",
      "  -0.15775621  0.03473853  0.17585827]\n",
      " [ 0.19415745 -0.12985002 -0.39827275  0.2418883   0.00921462  0.01619588\n",
      "   0.30461273 -0.2481349  -0.26224282]\n",
      " [ 0.5246599  -0.8413356   0.5762566   0.44929504 -0.59886014 -0.25734884\n",
      "  -0.2198478  -0.32053903 -0.75139415]\n",
      " [-3.11896    -0.3785908   0.5548894  -0.18161464  0.43100354  0.11317041\n",
      "  -0.26561683  0.40448576 -0.01872138]\n",
      " [-0.14561751 -0.21295217 -0.1286608  -0.06174336  0.02602147  0.3102729\n",
      "   0.22222832 -0.3558201   0.31119978]\n",
      " [ 0.14432487 -0.53388464  0.01801007  0.3989324  -0.0676719  -0.7453851\n",
      "   0.00723561 -0.6209259   0.47188827]\n",
      " [ 0.02208322  0.42512694 -0.6961117  -0.44147167 -1.5594134   0.6496739\n",
      "  -0.37163034  0.79535836 -0.95915496]\n",
      " [ 0.12435895  0.29011706 -0.40753394  0.7825638   0.22636327  1.0137787\n",
      "   0.36623028 -1.4066767  -0.37715515]\n",
      " [-1.6174765   0.7174911  -0.35559958 -0.60794127  0.45160326 -1.5531025\n",
      "  -0.6477845  -0.18110183  0.637974  ]\n",
      " [-0.4496131   0.06857408  0.11384646  0.08910856 -0.0655965  -0.5762148\n",
      "   0.47570777 -0.6159251   0.33101344]\n",
      " [ 0.06228692  0.12204177  0.80823565  0.21456888 -0.86923087 -0.68378294\n",
      "   0.39955     0.18719558 -0.44332546]\n",
      " [ 0.31147447 -0.09072841 -0.82119924  0.00906722  0.01476513 -0.54061043\n",
      "   0.47487888  1.0249605   0.50362617]\n",
      " [-0.5145961  -0.14358582 -0.2842003  -0.28748354  0.19448128  0.02569862\n",
      "  -0.48539078  0.06695937  0.08615766]\n",
      " [-0.58625257 -0.43139198 -0.01844748  0.29948953  0.22777739 -0.11489628\n",
      "  -0.05437197  0.20714101  0.01906972]\n",
      " [-0.25346342 -0.9987744  -1.4299465   0.56007344  0.11054536  0.4690253\n",
      "  -0.06517731  0.21547319 -0.40867832]\n",
      " [ 0.6050545   0.13073938 -2.0167954   0.5700615   0.10154894 -0.6107959\n",
      "  -0.4450155  -0.5148159  -0.28611958]\n",
      " [-0.79165345  0.6739752  -0.3656937  -1.933596   -1.208622    0.5039885\n",
      "   0.04753159  0.94199955 -1.183993  ]\n",
      " [-2.1812763   0.23708679 -0.27103236 -2.5234327  -0.19054231  0.1430915\n",
      "   0.9107645   0.9526162  -0.4759086 ]\n",
      " [-0.6906872   0.07556827  0.3944923  -0.4244136  -0.4125345   0.4955045\n",
      "  -0.04482872  0.3092198   0.42634392]\n",
      " [ 0.24301116 -1.6948341   0.1001814  -0.01285768  0.49295512 -0.17522018\n",
      "  -1.0031097   0.2576193   0.00843963]\n",
      " [ 1.0111136  -0.3124098  -0.70313436 -1.906146   -1.4382081  -1.6757282\n",
      "   0.393019    0.02642247  0.29187405]\n",
      " [ 0.08583514  0.30313453  0.12078798 -0.3404853  -0.01147961 -0.28745073\n",
      "  -0.27333593  0.03411216  0.08625023]\n",
      " [-0.4557584   0.09913496 -0.11653902  0.9240478  -0.1606773  -2.3943167\n",
      "   0.28636813  0.04825651 -0.4546781 ]\n",
      " [ 0.02136926 -0.3420322   0.19920442  0.06443296 -0.3630598  -0.17343038\n",
      "   0.3669725  -0.17543016  0.5641138 ]\n",
      " [-0.09394352  0.02984536  0.51160365 -0.17135285 -0.47723162  0.2954207\n",
      "  -1.0809524  -0.76768214  0.23619261]\n",
      " [ 0.56178796 -1.0942333  -0.99499595 -0.08949732 -0.302272    0.3025519\n",
      "   0.52532554  0.31794855  0.3092855 ]]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "num_of_layers = len(model.layers)\n",
    "\n",
    "weights = []\n",
    "bias = []\n",
    "activation = []\n",
    "for layer in model.layers:\n",
    "    if 'conv2d' in layer.get_config()['name']:\n",
    "        w, b = layer.get_weights()\n",
    "        print(w.shape)\n",
    "        print(b.shape)\n",
    "        continue\n",
    "    if 'activation' in layer.get_config()['name']:\n",
    "        continue\n",
    "    if 'flatten' in layer.get_config()['name']:\n",
    "        continue\n",
    "    if 'dropout' in layer.get_config()['name']:\n",
    "        continue\n",
    "    w, b = layer.get_weights()\n",
    "    activation.append(layer.get_config()['activation'])\n",
    "    weights.append(w)\n",
    "    bias.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998\n",
      "0.99733335\n",
      "[[0.0, 1.0], [<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: shape=(), dtype=float32, numpy=20.185328>], [<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: shape=(), dtype=float32, numpy=77.4887>]]\n"
     ]
    }
   ],
   "source": [
    "#Verify understanding of model weights, bias & activation function \n",
    "software_prediction = []    \n",
    "\n",
    "def dot_product(a, b, n):\n",
    "    total = []\n",
    "    for i in range(0, b.shape[1]):\n",
    "        s = 0\n",
    "        for j in range(0, n):\n",
    "            s += a[j] * b[j][i]\n",
    "        total.append(s)\n",
    "    return total\n",
    "\n",
    "def add(a, b, n):\n",
    "    total = []\n",
    "    for i in range(0, n):\n",
    "        total.append(a[i] + b[i])\n",
    "    return total\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "#minmax = [[float('inf'), float('-inf')] for x in range(0, num_of_layers)]\n",
    "\n",
    "for line in X:\n",
    "    in_data = line\n",
    "    for i in range(0, num_of_layers):\n",
    "#       minmax[i][0] = min(min(in_data), minmax[i][0])\n",
    "#        minmax[i][1] = max(max(in_data), minmax[i][1])\n",
    "        pdt = np.matmul(in_data, weights[i], dtype=np.float32)\n",
    "        in_data = add(pdt, bias[i], len(bias[i]))\n",
    "        if activation[i] == 'relu':\n",
    "            in_data = tf.keras.activations.relu(in_data)\n",
    "        elif activation[i] == 'softmax':\n",
    "            in_data = softmax(in_data)\n",
    "    software_prediction.append(in_data)\n",
    "\n",
    "software_prediction = np.array(software_prediction)\n",
    "m.update_state(prediction, software_prediction)\n",
    "print(m.result().numpy())\n",
    "m.update_state(software_prediction, y)\n",
    "print(m.result().numpy())\n",
    "print(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "for i in range(0, len(prediction)):\n",
    "    # absolute tolerance up to 1e-06\n",
    "    if not np.isclose(prediction[i], software_prediction[i], rtol=1e-05, atol=1e-06, equal_nan=False).all():\n",
    "        print(prediction[i])\n",
    "        print(software_prediction[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 136, 148, 80, 52, 68, -172, 69, 49, 133, -795, -37, 36, 5, 31, -412, -114, 15, 79, -131, -149, -64, 154, -201, -556, -176, 61, 257, 21, -116, 5, -23, 143, \\\n",
      " 144, 56, -72, 189, 34, 46, -25, -33, -214, -96, -54, -136, 108, 73, 182, 17, 31, -23, -36, -110, -254, 33, 171, 60, 19, -432, -79, 77, 25, -87, 7, -279, \\\n",
      " 102, 70, -78, 73, -242, 164, -17, -101, 146, 141, -32, 4, -177, -103, -90, 29, 206, -209, -72, -4, -364, -514, -93, -69, 100, 25, -179, 30, -29, 50, 130, -253, \\\n",
      " -196, -157, -65, -305, -235, -59, -177, 61, 114, -46, -15, 101, -112, 199, -155, 22, 54, 2, -73, 76, 142, 145, -493, -643, -108, -3, -486, -86, 235, 16, -43, -22, \\\n",
      " 57, 60, 67, -109, 172, 86, 162, 2, -152, 109, 6, -17, -397, 57, 115, -16, -221, 3, 49, 58, 28, 25, -308, -48, -105, 125, -366, -2, -40, -92, -121, -77, \\\n",
      " 241, 35, 14, -69, -290, -81, -101, 4, -65, 28, 79, -190, 165, 258, -396, -146, -174, -137, 6, -29, 119, -155, 128, 36, 126, -44, -427, -73, -610, -44, 75, 77, \\\n",
      " -264, -3, 52, -33, -31, 100, -40, 77, -56, -67, 56, 1, -94, 93, -165, 121, 101, 121, -123, -13, -16, -113, 12, 232, -11, -255, 100, -69, 73, 93, -275, 133, \\\n",
      " -44, -63, 64, -4, -120, -22, 8, -63, -81, 103, -90, -158, 202, -358, -46, -157, 47, 261, 17, 52, 54, -131, 240, 242, 78, 65, 6, 8, 12, -44, -195, 81, \\\n",
      " -270, -233, -31, 352, 32, -69, 44, -66, -191, -4, 79, 120, -244, -96, 162, 84, -113, 128, 21, 4, -104, -72, -301, -121, 108, 2, 74, 21, -115, 143, 60, 78, \\\n"
     ]
    }
   ],
   "source": [
    "minimum = float('inf')\n",
    "maximum = float('-inf')\n",
    "i = 2\n",
    "s = '{'\n",
    "for x in bias[i]:\n",
    "    s = s + ' ' + str(int(255 * x)) + ','\n",
    "s = s[:-1] + '}'\n",
    "print(s)\n",
    "for j in range(0, weights[i].shape[1]):\n",
    "    s = '{'\n",
    "    for k in range(0, weights[i].shape[0]):\n",
    "        s = s + ' ' + str(int(weights[i][k][j] * 255)) + ','\n",
    "    s = s[:-1] + '},\\n'\n",
    "    #print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.4.1\n",
      "Uninstalling tensorflow-2.4.1:\n",
      "  Successfully uninstalled tensorflow-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow\n",
    "!pip install -q tf-nightly\n",
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "(19194, 300)\n",
      "(19194, 9)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./ML_stuff/jeff_model')\n",
    "\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('./ML_stuff/input.csv', delimiter=',')\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "X = data.transpose()[:-1].transpose()\n",
    "y = to_categorical(data.transpose()[-1])\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_1 (QuantizeLa (None, 300)               3         \n",
      "_________________________________________________________________\n",
      "quant_dense_3 (QuantizeWrapp (None, 128)               38533     \n",
      "_________________________________________________________________\n",
      "quant_dense_4 (QuantizeWrapp (None, 32)                4133      \n",
      "_________________________________________________________________\n",
      "quant_dense_5 (QuantizeWrapp (None, 9)                 302       \n",
      "=================================================================\n",
      "Total params: 42,971\n",
      "Trainable params: 42,953\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 2s 5ms/step - loss: 1.7646 - accuracy: 0.3655 - val_loss: 1.2533 - val_accuracy: 0.5270\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 1.1669 - accuracy: 0.5496 - val_loss: 1.0355 - val_accuracy: 0.6007\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.9575 - accuracy: 0.6259 - val_loss: 0.8970 - val_accuracy: 0.6309\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.8222 - accuracy: 0.6720 - val_loss: 0.8733 - val_accuracy: 0.6483\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.7264 - accuracy: 0.7189 - val_loss: 0.7275 - val_accuracy: 0.7114\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.6461 - accuracy: 0.7546 - val_loss: 0.6406 - val_accuracy: 0.7377\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 2s 6ms/step - loss: 0.5633 - accuracy: 0.7872 - val_loss: 0.5710 - val_accuracy: 0.7729\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.4907 - accuracy: 0.8141 - val_loss: 0.5372 - val_accuracy: 0.7947\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.4313 - accuracy: 0.8428 - val_loss: 0.4510 - val_accuracy: 0.8250\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 1s 6ms/step - loss: 0.3686 - accuracy: 0.8679 - val_loss: 0.4070 - val_accuracy: 0.8526\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.3189 - accuracy: 0.8872 - val_loss: 0.3661 - val_accuracy: 0.8612\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.2903 - accuracy: 0.8989 - val_loss: 0.3557 - val_accuracy: 0.8591\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.2577 - accuracy: 0.9142 - val_loss: 0.2955 - val_accuracy: 0.8893\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.2211 - accuracy: 0.9260 - val_loss: 0.3200 - val_accuracy: 0.8771\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.2049 - accuracy: 0.9291 - val_loss: 0.2632 - val_accuracy: 0.9039\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.1807 - accuracy: 0.9431 - val_loss: 0.2441 - val_accuracy: 0.9070\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 3s 11ms/step - loss: 0.1672 - accuracy: 0.9484 - val_loss: 0.2771 - val_accuracy: 0.8875\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1547 - accuracy: 0.9498 - val_loss: 0.2413 - val_accuracy: 0.9044\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1450 - accuracy: 0.9531 - val_loss: 0.2317 - val_accuracy: 0.9073\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1300 - accuracy: 0.9597 - val_loss: 0.2044 - val_accuracy: 0.9198\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.1156 - accuracy: 0.9642 - val_loss: 0.2061 - val_accuracy: 0.9192\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1082 - accuracy: 0.9678 - val_loss: 0.1954 - val_accuracy: 0.9299\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.1119 - accuracy: 0.9645 - val_loss: 0.2136 - val_accuracy: 0.9133\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.1003 - accuracy: 0.9700 - val_loss: 0.1832 - val_accuracy: 0.9278\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0893 - accuracy: 0.9739 - val_loss: 0.2005 - val_accuracy: 0.9172\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0755 - accuracy: 0.9795 - val_loss: 0.1658 - val_accuracy: 0.9344\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0753 - accuracy: 0.9792 - val_loss: 0.1697 - val_accuracy: 0.9349\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.0701 - accuracy: 0.9812 - val_loss: 0.1685 - val_accuracy: 0.9385\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.0598 - accuracy: 0.9848 - val_loss: 0.1542 - val_accuracy: 0.9401\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.0617 - accuracy: 0.9841 - val_loss: 0.1527 - val_accuracy: 0.9417\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0472 - accuracy: 0.9907 - val_loss: 0.1361 - val_accuracy: 0.9461\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0446 - accuracy: 0.9912 - val_loss: 0.1490 - val_accuracy: 0.9414\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0511 - accuracy: 0.9879 - val_loss: 0.1342 - val_accuracy: 0.9502\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0428 - accuracy: 0.9901 - val_loss: 0.1433 - val_accuracy: 0.9443\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0396 - accuracy: 0.9913 - val_loss: 0.1397 - val_accuracy: 0.9432\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.0341 - accuracy: 0.9924 - val_loss: 0.1292 - val_accuracy: 0.9505\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0243 - accuracy: 0.9974 - val_loss: 0.1424 - val_accuracy: 0.9424\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.0278 - accuracy: 0.9966 - val_loss: 0.1332 - val_accuracy: 0.9505\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0248 - accuracy: 0.9962 - val_loss: 0.1466 - val_accuracy: 0.9463\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0235 - accuracy: 0.9956 - val_loss: 0.1506 - val_accuracy: 0.9435\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9970 - val_loss: 0.1288 - val_accuracy: 0.9523\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 2s 7ms/step - loss: 0.0235 - accuracy: 0.9953 - val_loss: 0.1791 - val_accuracy: 0.9362\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.0375 - accuracy: 0.9906 - val_loss: 0.1199 - val_accuracy: 0.9557\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.0142 - accuracy: 0.9987 - val_loss: 0.1183 - val_accuracy: 0.9555\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0122 - accuracy: 0.9993 - val_loss: 0.1160 - val_accuracy: 0.9578\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0143 - accuracy: 0.9982 - val_loss: 0.1343 - val_accuracy: 0.9513\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 1s 4ms/step - loss: 0.0124 - accuracy: 0.9990 - val_loss: 0.1203 - val_accuracy: 0.9565\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 1s 5ms/step - loss: 0.0087 - accuracy: 0.9997 - val_loss: 0.1144 - val_accuracy: 0.9586\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 1s 2ms/step - loss: 0.0089 - accuracy: 0.9994 - val_loss: 0.1441 - val_accuracy: 0.9484\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 1s 3ms/step - loss: 0.0424 - accuracy: 0.9864 - val_loss: 0.1615 - val_accuracy: 0.9456\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'],)\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=50, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_1 (QuantizeLa (None, 300)               3         \n",
      "_________________________________________________________________\n",
      "quant_dense_3 (QuantizeWrapp (None, 128)               38533     \n",
      "_________________________________________________________________\n",
      "quant_dense_4 (QuantizeWrapp (None, 32)                4133      \n",
      "_________________________________________________________________\n",
      "quant_dense_5 (QuantizeWrapp (None, 9)                 302       \n",
      "=================================================================\n",
      "Total params: 42,971\n",
      "Trainable params: 42,953\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n",
      "{'name': 'quantize_layer_1', 'trainable': True, 'dtype': 'float32', 'quantizer': {'class_name': 'AllValuesQuantizer', 'config': {'num_bits': 8, 'per_axis': False, 'symmetric': False, 'narrow_range': False}}}\n",
      "[0.0, 0.0, -1]\n",
      "{'name': 'quant_dense_3', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'batch_input_shape': (None, 300), 'dtype': 'float32', 'units': 128, 'activation': {'class_name': 'QuantizeAwareActivation', 'config': {'activation': 'relu'}}, 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, 'quantize_config': {'class_name': 'Default8BitQuantizeConfig', 'config': {'weight_attrs': ['kernel'], 'activation_attrs': ['activation'], 'quantize_output': False}}}\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.11697314, -0.00660339,  0.03524594, ..., -0.04718073,\n",
      "         0.11346601,  0.07960796],\n",
      "       [-0.09740241, -0.02851595,  0.1005156 , ...,  0.04779524,\n",
      "        -0.01044709,  0.10267749],\n",
      "       [ 0.07155742, -0.09250253, -0.03029231, ..., -0.05491237,\n",
      "         0.02534585,  0.00212425],\n",
      "       ...,\n",
      "       [-0.03555217, -0.06361726, -0.01430928, ...,  0.10262738,\n",
      "         0.0071221 ,  0.00090635],\n",
      "       [ 0.03364161,  0.01123467, -0.08563924, ...,  0.09718911,\n",
      "        -0.0797746 , -0.0666181 ],\n",
      "       [-0.07369446,  0.05399586,  0.11432272, ..., -0.00240273,\n",
      "        -0.11249894, -0.10027408]], dtype=float32), -1, -6.0, 6.0, -6.0, 6.0]\n",
      "{'name': 'quant_dense_4', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 32, 'activation': {'class_name': 'QuantizeAwareActivation', 'config': {'activation': 'relu'}}, 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, 'quantize_config': {'class_name': 'Default8BitQuantizeConfig', 'config': {'weight_attrs': ['kernel'], 'activation_attrs': ['activation'], 'quantize_output': False}}}\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([[-0.02024443,  0.18506095,  0.07441905, ...,  0.19247308,\n",
      "         0.04317532,  0.03721507],\n",
      "       [ 0.12057048, -0.07496653, -0.17890224, ...,  0.15051743,\n",
      "        -0.04580376, -0.05456993],\n",
      "       [-0.093987  , -0.06568161, -0.03546025, ...,  0.05171563,\n",
      "        -0.01644851,  0.12698245],\n",
      "       ...,\n",
      "       [ 0.11897269,  0.07724527, -0.09532347, ..., -0.13340427,\n",
      "         0.19013041,  0.15644145],\n",
      "       [ 0.18642217, -0.06426859, -0.06716642, ...,  0.08201846,\n",
      "        -0.1293715 , -0.16034254],\n",
      "       [ 0.07359567,  0.07784465, -0.08089886, ...,  0.12423024,\n",
      "         0.01446839,  0.04913411]], dtype=float32), -1, -6.0, 6.0, -6.0, 6.0]\n",
      "{'name': 'quant_dense_5', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 9, 'activation': {'class_name': 'QuantizeAwareActivation', 'config': {'activation': 'softmax'}}, 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, 'quantize_config': {'class_name': 'Default8BitQuantizeConfig', 'config': {'weight_attrs': ['kernel'], 'activation_attrs': ['activation'], 'quantize_output': False}}}\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.30469042, -0.03677821, -0.15943198, -0.36402974,  0.19170049,\n",
      "         0.10405913,  0.18420383,  0.25195035, -0.02980405],\n",
      "       [-0.00766861,  0.07598507,  0.10549524,  0.13684586, -0.02244017,\n",
      "         0.26788405, -0.03595626,  0.1834254 ,  0.13834777],\n",
      "       [ 0.01916969, -0.01495624, -0.05466089,  0.23505989, -0.10503229,\n",
      "        -0.12396976,  0.02645287, -0.16858706, -0.31764406],\n",
      "       [ 0.35310087,  0.30364975,  0.07950333, -0.1292921 ,  0.19997492,\n",
      "        -0.3407368 ,  0.19476166,  0.04068246,  0.06865466],\n",
      "       [-0.10998842,  0.30092236,  0.2253634 ,  0.24063477,  0.30360237,\n",
      "         0.14192167, -0.00377318, -0.02293777,  0.21143511],\n",
      "       [ 0.05624878, -0.19085228, -0.24177675,  0.05742544,  0.07975489,\n",
      "         0.27578774, -0.27275744, -0.09880948,  0.04779053],\n",
      "       [ 0.12821278,  0.38148132,  0.02156833, -0.12487546, -0.26641262,\n",
      "        -0.31352812,  0.13138053, -0.12288699,  0.3809401 ],\n",
      "       [-0.01991794, -0.1538446 , -0.3292256 ,  0.32190886,  0.2651349 ,\n",
      "        -0.1237641 ,  0.26502004, -0.22888686,  0.26045528],\n",
      "       [ 0.1758019 ,  0.18110552, -0.21544364, -0.11762255, -0.27383193,\n",
      "         0.04855502,  0.28523043,  0.16233978,  0.33733347],\n",
      "       [-0.15357262,  0.19446805,  0.22317657,  0.20348224, -0.23667148,\n",
      "         0.11430585,  0.01653758, -0.3246774 , -0.2155166 ],\n",
      "       [-0.35286102, -0.23858225,  0.06339332,  0.24567774,  0.2738718 ,\n",
      "        -0.2825008 , -0.10539556,  0.33378026,  0.05639884],\n",
      "       [ 0.20426962, -0.04866901, -0.12629098, -0.17588437, -0.25349417,\n",
      "         0.2428756 ,  0.3317149 ,  0.2600964 ,  0.28004572],\n",
      "       [ 0.272015  , -0.27903488, -0.304785  , -0.24431081, -0.00780615,\n",
      "        -0.10538444, -0.13123187,  0.06638864,  0.14981994],\n",
      "       [ 0.28317365,  0.2978507 , -0.03026766, -0.14557904, -0.21220116,\n",
      "         0.05676958,  0.1768311 , -0.04564053,  0.06010845],\n",
      "       [-0.15485306, -0.16806608, -0.20498931, -0.35453892, -0.07907385,\n",
      "        -0.36190107, -0.32630947,  0.16807595, -0.33954793],\n",
      "       [ 0.09183094, -0.07149106,  0.2344974 ,  0.2953159 ,  0.20358828,\n",
      "         0.3044285 ,  0.3647798 ,  0.01197127,  0.09970093],\n",
      "       [ 0.24113992,  0.17845109, -0.23471959,  0.35056004,  0.09650743,\n",
      "         0.13629344, -0.03733557, -0.13715534, -0.26529214],\n",
      "       [-0.28347617, -0.05468461,  0.03856167,  0.24279734,  0.11219972,\n",
      "         0.0109522 , -0.17130518, -0.13519897, -0.326167  ],\n",
      "       [-0.07419652, -0.10600263,  0.03682837,  0.2672259 ,  0.21156362,\n",
      "         0.01228809,  0.21590039, -0.02591521,  0.22069845],\n",
      "       [-0.13612235, -0.35125506, -0.19090337, -0.14297776, -0.07358882,\n",
      "         0.19777802, -0.18704799, -0.1660518 ,  0.08571884],\n",
      "       [-0.11910656, -0.35202947, -0.15867962, -0.13971713,  0.32194278,\n",
      "         0.22945169,  0.11807731, -0.05414814,  0.22042128],\n",
      "       [-0.33720887, -0.08890733,  0.30629912, -0.33930287,  0.253853  ,\n",
      "         0.08426148,  0.14539883, -0.3070753 ,  0.03832132],\n",
      "       [-0.24577175, -0.19172157,  0.20429268, -0.2760744 ,  0.21507433,\n",
      "         0.24752495, -0.23657316, -0.2671106 , -0.24439773],\n",
      "       [ 0.25269613,  0.15524736,  0.25215444, -0.27289143,  0.24240121,\n",
      "         0.30270895,  0.22377804,  0.03273615, -0.3112361 ],\n",
      "       [-0.38241243, -0.02751276, -0.285717  , -0.198774  ,  0.13986644,\n",
      "         0.1963292 , -0.31834224, -0.12409791, -0.03536543],\n",
      "       [-0.03171182, -0.37540606,  0.18219402,  0.01939368, -0.06487533,\n",
      "        -0.28524676,  0.37905332,  0.16025737,  0.2931536 ],\n",
      "       [ 0.23187384, -0.1788555 , -0.03830108,  0.10080561,  0.25823984,\n",
      "        -0.221476  ,  0.24405417, -0.28204688,  0.08851451],\n",
      "       [-0.00410119,  0.16623506, -0.12541428,  0.12534413,  0.34197208,\n",
      "         0.14738712, -0.260949  ,  0.20323339, -0.19940469],\n",
      "       [-0.21053055,  0.0345878 , -0.13918085,  0.35371408,  0.1211597 ,\n",
      "         0.20340195,  0.10595602,  0.04715136, -0.11031157],\n",
      "       [-0.01552656,  0.082385  , -0.07217777, -0.2057248 ,  0.17451343,\n",
      "         0.19986317,  0.24525234,  0.34396103,  0.0259631 ],\n",
      "       [ 0.14633176,  0.00211024,  0.36138347,  0.16520551,  0.11591628,\n",
      "        -0.35480013, -0.28272006,  0.06984362,  0.37302586],\n",
      "       [-0.16148101,  0.30493233, -0.09044451,  0.34552786, -0.24074912,\n",
      "        -0.19528836, -0.01582041,  0.17545399,  0.00530109]],\n",
      "      dtype=float32), -1, -6.0, 6.0, -6.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "q_aware_model.summary()\n",
    "num_of_layers = len(model.layers)\n",
    "\n",
    "weights = []\n",
    "bias = []\n",
    "activation = []\n",
    "for layer in q_aware_model.layers:\n",
    "    print(layer.get_config())\n",
    "    w = layer.get_weights()\n",
    "    print(w)\n",
    "    #activation.append(layer.get_config()['activation'])\n",
    "    #weights.append(w)\n",
    "#bias.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_3_layer_call_fn, dense_3_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jeffe\\AppData\\Local\\Temp\\tmpwxvgvbu5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\jeffe\\AppData\\Local\\Temp\\tmpwxvgvbu5\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "#interpreter.allocate_tensors()\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'dense_3_input',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 300]),\n",
       "  'shape_signature': array([ -1, 300]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars;sequential_1/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1/resource',\n",
       "  'index': 1,\n",
       "  'shape': array([  1, 300]),\n",
       "  'shape_signature': array([ -1, 300]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (3.921568847431445e-09, -1),\n",
       "  'quantization_parameters': {'scales': array([0.], dtype=float32),\n",
       "   'zero_points': array([-1]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quant_dense_3/MatMul;sequential_1/quant_dense_3/LastValueQuant/FakeQuantWithMinMaxVars',\n",
       "  'index': 2,\n",
       "  'shape': array([128, 300]),\n",
       "  'shape_signature': array([128, 300]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.04724409431219101, 0),\n",
       "  'quantization_parameters': {'scales': array([0.04724409], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quant_dense_3/MatMul;sequential_1/quant_dense_3/Relu;sequential_1/quant_dense_3/BiasAdd',\n",
       "  'index': 3,\n",
       "  'shape': array([  1, 128]),\n",
       "  'shape_signature': array([ -1, 128]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0470588244497776, 0),\n",
       "  'quantization_parameters': {'scales': array([0.04705882], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quant_dense_4/MatMul;sequential_1/quant_dense_4/LastValueQuant/FakeQuantWithMinMaxVars',\n",
       "  'index': 4,\n",
       "  'shape': array([ 32, 128]),\n",
       "  'shape_signature': array([ 32, 128]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.04724409431219101, 0),\n",
       "  'quantization_parameters': {'scales': array([0.04724409], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quant_dense_4/MatMul;sequential_1/quant_dense_4/Relu;sequential_1/quant_dense_4/BiasAdd',\n",
       "  'index': 5,\n",
       "  'shape': array([ 1, 32]),\n",
       "  'shape_signature': array([-1, 32]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0470588244497776, 0),\n",
       "  'quantization_parameters': {'scales': array([0.04705882], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quant_dense_5/MatMul;sequential_1/quant_dense_5/LastValueQuant/FakeQuantWithMinMaxVars',\n",
       "  'index': 6,\n",
       "  'shape': array([ 9, 32]),\n",
       "  'shape_signature': array([ 9, 32]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.04724409431219101, 0),\n",
       "  'quantization_parameters': {'scales': array([0.04724409], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quant_dense_5/MatMul;sequential_1/quant_dense_5/BiasAdd',\n",
       "  'index': 7,\n",
       "  'shape': array([1, 9]),\n",
       "  'shape_signature': array([-1,  9]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.0470588244497776, 0),\n",
       "  'quantization_parameters': {'scales': array([0.04705882], dtype=float32),\n",
       "   'zero_points': array([0]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'sequential_1/quant_dense_5/Softmax',\n",
       "  'index': 8,\n",
       "  'shape': array([1, 9]),\n",
       "  'shape_signature': array([-1,  9]),\n",
       "  'dtype': numpy.int8,\n",
       "  'quantization': (0.00390625, -128),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([-128]),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}},\n",
       " {'name': 'Identity',\n",
       "  'index': 9,\n",
       "  'shape': array([1, 9]),\n",
       "  'shape_signature': array([-1,  9]),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_tensor_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_3_input\n",
      "[  1 300]\n",
      "(0.0, 0)\n",
      "sequential_1/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars;sequential_1/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1/resource\n",
      "[  1 300]\n",
      "(3.921568847431445e-09, -1)\n",
      "sequential_1/quant_dense_3/MatMul;sequential_1/quant_dense_3/LastValueQuant/FakeQuantWithMinMaxVars\n",
      "[128 300]\n",
      "(0.04724409431219101, 0)\n",
      "[[-2 -2  2 ... -1  1 -2]\n",
      " [ 0 -1 -2 ... -1  0  1]\n",
      " [ 1  2 -1 ...  0 -2  2]\n",
      " ...\n",
      " [-1  1 -1 ...  2  2  0]\n",
      " [ 2  0  1 ...  0 -2 -2]\n",
      " [ 2  2  0 ...  0 -1 -2]]\n",
      "sequential_1/quant_dense_3/MatMul;sequential_1/quant_dense_3/Relu;sequential_1/quant_dense_3/BiasAdd\n",
      "[  1 128]\n",
      "(0.0470588244497776, 0)\n",
      "sequential_1/quant_dense_4/MatMul;sequential_1/quant_dense_4/LastValueQuant/FakeQuantWithMinMaxVars\n",
      "[ 32 128]\n",
      "(0.04724409431219101, 0)\n",
      "[[ 0  3 -2 ...  3  4  2]\n",
      " [ 4 -2 -1 ...  2 -1  2]\n",
      " [ 2 -4 -1 ... -2 -1 -2]\n",
      " ...\n",
      " [ 4  3  1 ... -3  2  3]\n",
      " [ 1 -1  0 ...  4 -3  0]\n",
      " [ 1 -1  3 ...  3 -3  1]]\n",
      "sequential_1/quant_dense_4/MatMul;sequential_1/quant_dense_4/Relu;sequential_1/quant_dense_4/BiasAdd\n",
      "[ 1 32]\n",
      "(0.0470588244497776, 0)\n",
      "sequential_1/quant_dense_5/MatMul;sequential_1/quant_dense_5/LastValueQuant/FakeQuantWithMinMaxVars\n",
      "[ 9 32]\n",
      "(0.04724409431219101, 0)\n",
      "[[-6  0  0  7 -2  1  3  0  4 -3 -7  4  6  6 -3  2  5 -6 -2 -3 -3 -7 -5  5\n",
      "  -8 -1  5  0 -4  0  3 -3]\n",
      " [-1  2  0  6  6 -4  8 -3  4  4 -5 -1 -6  6 -4 -2  4 -1 -2 -7 -7 -2 -4  3\n",
      "  -1 -8 -4  4  1  2  0  6]\n",
      " [-3  2 -1  2  5 -5  0 -7 -5  5  1 -3 -6 -1 -4  5 -5  1  1 -4 -3  6  4  5\n",
      "  -6  4 -1 -3 -3 -2  8 -2]\n",
      " [-8  3  5 -3  5  1 -3  7 -2  4  5 -4 -5 -3 -8  6  7  5  6 -3 -3 -7 -6 -6\n",
      "  -4  0  2  3  7 -4  3  7]\n",
      " [ 4  0 -2  4  6  2 -6  6 -6 -5  6 -5  0 -4 -2  4  2  2  4 -2  7  5  5  5\n",
      "   3 -1  5  7  3  4  2 -5]\n",
      " [ 2  6 -3 -7  3  6 -7 -3  1  2 -6  5 -2  1 -8  6  3  0  0  4  5  2  5  6\n",
      "   4 -6 -5  3  4  4 -8 -4]\n",
      " [ 4 -1  1  4  0 -6  3  6  6  0 -2  7 -3  4 -7  8 -1 -4  5 -4  2  3 -5  5\n",
      "  -7  8  5 -6  2  5 -6  0]\n",
      " [ 5  4 -4  1  0 -2 -3 -5  3 -7  7  6  1 -1  4  0 -3 -3 -1 -4 -1 -6 -6  1\n",
      "  -3  3 -6  4  1  7  1  4]\n",
      " [-1  3 -7  1  4  1  8  6  7 -5  1  6  3  1 -7  2 -6 -7  5  2  5  1 -5 -7\n",
      "  -1  6  2 -4 -2  1  8  0]]\n",
      "sequential_1/quant_dense_5/MatMul;sequential_1/quant_dense_5/BiasAdd\n",
      "[1 9]\n",
      "(0.0470588244497776, 0)\n",
      "sequential_1/quant_dense_5/Softmax\n",
      "[1 9]\n",
      "(0.00390625, -128)\n",
      "Identity\n",
      "[1 9]\n",
      "(0.0, 0)\n"
     ]
    }
   ],
   "source": [
    "for layer in interpreter.get_tensor_details():\n",
    "    # to store layer's metadata in group's metadata\n",
    "    print(layer['name'])\n",
    "    print(layer['shape'])\n",
    "    # grp.attrs[\"dtype\"] = all_layers_details[i]['dtype']\n",
    "    print(layer['quantization'])\n",
    "\n",
    "    try:\n",
    "        x = interpreter.get_tensor(layer['index'])\n",
    "        print(np.array2string(x, formatter={'float_kind':lambda x: \"%.2f\" % x}))\n",
    "    except ValueError:\n",
    "           pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_3_input\n",
      "[  1 300]\n",
      "(0.0, 0)\n",
      "[[0.00 0.00 0.00 0.00 0.00 0.00 nan nan 0.00\n",
      "  45865672592639313129451570576947675136.00 14660303127378680281563136.00\n",
      "  14274142130452184432640.00 0.00 1090262563567962203967455232.00 0.00\n",
      "  0.00 269072133705621831680.00 2218514972672.00 0.00\n",
      "  302625243599930827651623682048.00 0.00 0.00 0.00\n",
      "  70975523162658881667072.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "  10259322674239835734016.00 16245716408460115968.00\n",
      "  4689377801507748948350074880.00 0.00 18319372764645100982632448.00 0.00\n",
      "  210960.28 0.00 0.00 215.28 206.02 10960498665481947862188699942912.00\n",
      "  228382216625445666816.00 265441172904574418789017313083392.00\n",
      "  268754910969856.00 243746448.00 236846151582949048320.00\n",
      "  4463274336913116347489671184384.00 0.00\n",
      "  295137591530954238550853287936.00 0.00 0.00\n",
      "  18060448592452711050463150080.00 0.00 273256083312565288960.00 0.00\n",
      "  41792617530109042135689854976.00 304812811342137791392577012891648.00\n",
      "  0.00 0.00 17110703305543122944.00 19364286054242061838699947098112.00\n",
      "  18028429767107234138502987776.00 0.00 0.00\n",
      "  4465098605974914822914302803968.00 17829594794108649472.00\n",
      "  1125720771663845711151104.00 609073920848747823104.00 0.00 0.00\n",
      "  557218.88 75550726227875318648488534736896.00 0.00\n",
      "  10052158883813444611473408.00 77782780475739643869298643959808.00 0.00\n",
      "  41483132520287697066965073920.00 304812811342137791392577012891648.00\n",
      "  2218515234816.00 0.00 41855158927314097609742091288576.00 0.00\n",
      "  140551952.00 260444653477887485658856494727168.00 0.00 0.00 0.00 0.00\n",
      "  0.00 75030805125127725186480603136.00\n",
      "  72084494117876253878770986909696.00 0.00\n",
      "  44063348979499230050643500597248.00 18886460541891018886655680446464.00\n",
      "  71849419648752362192896.00 0.00 1157837045393656507638218752.00 0.00\n",
      "  0.00 17443123136937680266959257600.00\n",
      "  18970332188478423014909270818816.00 0.00\n",
      "  74085410967089636562924732416.00 0.00 0.00 0.00\n",
      "  70975523162658881667072.00 0.00 0.00 0.00 138.07\n",
      "  1006034803914380686127005696.00 4333020567308270714224640.00\n",
      "  67418737802948621893632.00 0.00 77781039622559398803287067066368.00\n",
      "  0.00 79309316911479729982845237323628544.00 0.00\n",
      "  72262322270238287371853566574592.00 0.00\n",
      "  308808086920224602301269016576.00 0.00 267995790986866327552.00 0.00\n",
      "  279041594268752367625779019776.00 0.00 0.00 152514330624.00\n",
      "  62776739364864.00 269145827372961890304.00 3849085440.00 0.00\n",
      "  18178540178292723563789587316736.00 4472574048062563942400.00 0.00\n",
      "  674589272925238077147970501869568.00 72436189140602836746240.00\n",
      "  72262438327116970376254338367488.00 0.00 9816564787673787531264.00\n",
      "  70975523162658881667072.00 79187686826739862800075848570372096.00 0.00\n",
      "  0.00 1108925282121286747013775360.00 0.00 264478532454448103424.00\n",
      "  71561207286999159930880.00 2218514972672.00 0.00\n",
      "  18465450225121797708185600.00 18464884140663035746320384.00 0.00 0.00\n",
      "  0.00 308808086920224602301269016576.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "  73027722220855979853460239548416.00 0.00 648173945629495266671329280.00\n",
      "  73968692050404665700851141050368.00 2218514972672.00 0.00\n",
      "  73027876963360890525994601938944.00 0.00 0.00 0.00\n",
      "  308808086920224602301269016576.00 0.00 0.00 0.00 0.00\n",
      "  12779033748458006642688.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "  -194587490626112175486989229901914046464.00 0.00\n",
      "  17748255305453440240208314368.00 0.00 295139272693422140144549363712.00\n",
      "  0.00 4158322154738882183168.00 1157705040493065042086854656.00 0.00\n",
      "  10540475523344139174874659684352.00 70947799003352788893696.00 0.00\n",
      "  -180832487775565307548899226708271431680.00 nan nan 0.00 0.00\n",
      "  282785312482169125296424697724928.00 17466493173663531008.00\n",
      "  4232407213034056974336.00 68887532778219980718080.00\n",
      "  274824919279634219008.00 0.00 67308791425245625974784.00\n",
      "  16533679603286278144.00 18056274020481854284094242816.00 0.00\n",
      "  9816562535873973846016.00 70975523162658881667072.00 0.00 0.00\n",
      "  1136184677490553211349781970944.00 75550808434831052443272414756864.00\n",
      "  17033010714412843008.00 305737475180231515121659477491712.00\n",
      "  18468696852078770589270016.00 1080278669166436530765430784.00\n",
      "  4545203541939854670316802408448.00 10.95 247213490701714063360.00\n",
      "  18179589525904149061913232277504.00 17443988510595666129444667392.00\n",
      "  73908555470378700374016.00 272339742064063903822774272.00\n",
      "  282832337009285005312.00 16279304679437891216080896.00\n",
      "  15520191184845623389061120.00 18278707090745218189361576597782528.00\n",
      "  19203339342025127027412964474880.00 264389146557156425728.00\n",
      "  4463268292284018274343797653504.00 1094708966759489154288975872.00 0.00\n",
      "  75327497244146976386321481728.00 67130534448394674372608.00\n",
      "  1006035394210191044832657408.00 4333020567308270714224640.00\n",
      "  67418737802948621893632.00 0.00 267995790986866327552.00 0.00 0.00\n",
      "  72262322270238287371853566574592.00 0.00 0.00 37935122481152.00\n",
      "  306006146854382670309446178045952.00 18468701463764789016657920.00 0.00\n",
      "  282975502219314462720.00 303041660662628750073479703922999296.00\n",
      "  272519093862606569472.00 17466499770733297664.00\n",
      "  934182153203111663245459456.00 1089665700716713257715367936.00\n",
      "  65336201216.00 75545856674673910922172818259968.00 0.00\n",
      "  69471906355068694167552.00 17443988510595666129444667392.00\n",
      "  73948909188292491909036249186304.00 18050896425649486475607146496.00\n",
      "  68274196840448.00 0.00 16631273258122280435712.00\n",
      "  18313731519723059680378880.00 69768328778942592843776.00 0.00\n",
      "  16631273258122280435712.00 18313731519723059680378880.00\n",
      "  69768328778942592843776.00 11128481540651089920.00\n",
      "  306006146854382670309446178045952.00 18468701463764789016657920.00 0.00\n",
      "  282975502219314462720.00 303041660662628750073479703922999296.00\n",
      "  272519093862606569472.00 17466499770733297664.00\n",
      "  934182153203111663245459456.00 1089665700716713257715367936.00\n",
      "  65336201216.00 75545856674673910922172818259968.00 0.00\n",
      "  69471906355068694167552.00 17443988510595666129444667392.00\n",
      "  73948909188292491909036249186304.00 18050896425649486475607146496.00\n",
      "  68274196840448.00 0.00 16631273258122280435712.00]]\n",
      "sequential_1/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars;sequential_1/quantize_layer_1/AllValuesQuantize/FakeQuantWithMinMaxVars/ReadVariableOp_1/resource\n",
      "[  1 300]\n",
      "(3.921568847431445e-09, -1)\n",
      "[[ 114   99  101   42   11   10    5  100  116  121  112  101   18    2\n",
      "    48    1   10 -118    1   10   97  115  101  113  117  101  110  116\n",
      "   105   97  108   95   49   47  113  117   97  110  116  105  122  101\n",
      "    95  108   97  121  101  114   95   49   47   65  108  108   86   97\n",
      "   108  117  101  115   81  117   97  110  116  105  122  101   47   70\n",
      "    97  107  101   81  117   97  110  116   87  105  116  104   77  105\n",
      "   110   77   97  120   86   97  114  115   47   82  101   97  100   86\n",
      "    97  114  105   97   98  108  101   79  112   95   49   47  114  101\n",
      "   115  111  117  114   99  101   18   11   80  108   97   99  101  104\n",
      "   111  108  100  101  114   42   11   10    5  115  104   97  112  101\n",
      "    18    2   58    0   42   11   10    5  100  116  121  112  101   18\n",
      "     2   48   20   10  -38    1   10   88  115  101  113  117  101  110\n",
      "   116  105   97  108   95   49   47  113  117   97  110  116  105  122\n",
      "   101   95  108   97  121  101  114   95   49   47   65  108  108   86\n",
      "    97  108  117  101  115   81  117   97  110  116  105  122  101   47\n",
      "    70   97  107  101   81  117   97  110  116   87  105  116  104   77\n",
      "   105  110   77   97  120   86   97  114  115   47   82  101   97  100\n",
      "    86   97  114  105   97   98  108  101   79  112   95   49   18   14\n",
      "    82  101   97  100   86   97  114  105   97   98  108  101   79  112\n",
      "    26   97  115  101  113  117  101  110  116  105   97  108   95   49\n",
      "    47  113  117   97  110  116  105  122  101   95  108   97  121  101\n",
      "   114   95   49   47   65  108]]\n",
      "sequential_1/quant_dense_3/MatMul;sequential_1/quant_dense_3/LastValueQuant/FakeQuantWithMinMaxVars\n",
      "[128 300]\n",
      "(0.04724409431219101, 0)\n",
      "[[-2 -2  2 ... -1  1 -2]\n",
      " [ 0 -1 -2 ... -1  0  1]\n",
      " [ 1  2 -1 ...  0 -2  2]\n",
      " ...\n",
      " [-1  1 -1 ...  2  2  0]\n",
      " [ 2  0  1 ...  0 -2 -2]\n",
      " [ 2  2  0 ...  0 -1 -2]]\n",
      "sequential_1/quant_dense_3/MatMul;sequential_1/quant_dense_3/Relu;sequential_1/quant_dense_3/BiasAdd\n",
      "[  1 128]\n",
      "(0.0470588244497776, 0)\n",
      "[[ 81 117  97 110 116  87 105 116 104  77 105 110  77  97 120  86  97 114\n",
      "  115  47  82 101  97 100  86  97 114 105  97  98 108 101  79 112  95  49\n",
      "   47 114 101 115 111 117 114  99 101  42  11  10   5 100 116 121 112 101\n",
      "   18   2  48   1  10 -57   2  10  71 115 101 113 117 101 110 116 105  97\n",
      "  108  95  49  47 113 117  97 110 116 105 122 101  95 108  97 121 101 114\n",
      "   95  49  47  65 108 108  86  97 108 117 101 115  81 117  97 110 116 105\n",
      "  122 101  47  70  97 107 101  81 117  97 110 116  87 105 116 104  77 105\n",
      "  110  77]]\n",
      "sequential_1/quant_dense_4/MatMul;sequential_1/quant_dense_4/LastValueQuant/FakeQuantWithMinMaxVars\n",
      "[ 32 128]\n",
      "(0.04724409431219101, 0)\n",
      "[[ 0  3 -2 ...  3  4  2]\n",
      " [ 4 -2 -1 ...  2 -1  2]\n",
      " [ 2 -4 -1 ... -2 -1 -2]\n",
      " ...\n",
      " [ 4  3  1 ... -3  2  3]\n",
      " [ 1 -1  0 ...  4 -3  0]\n",
      " [ 1 -1  3 ...  3 -3  1]]\n",
      "sequential_1/quant_dense_4/MatMul;sequential_1/quant_dense_4/Relu;sequential_1/quant_dense_4/BiasAdd\n",
      "[ 1 32]\n",
      "(0.0470588244497776, 0)\n",
      "[[ 114   99  101   42   11   10    5  100  116  121  112  101   18    2\n",
      "    48    1   10 -118    1   10   97  115  101  113  117  101  110  116\n",
      "   105   97  108   95]]\n",
      "sequential_1/quant_dense_5/MatMul;sequential_1/quant_dense_5/LastValueQuant/FakeQuantWithMinMaxVars\n",
      "[ 9 32]\n",
      "(0.04724409431219101, 0)\n",
      "[[-6  0  0  7 -2  1  3  0  4 -3 -7  4  6  6 -3  2  5 -6 -2 -3 -3 -7 -5  5\n",
      "  -8 -1  5  0 -4  0  3 -3]\n",
      " [-1  2  0  6  6 -4  8 -3  4  4 -5 -1 -6  6 -4 -2  4 -1 -2 -7 -7 -2 -4  3\n",
      "  -1 -8 -4  4  1  2  0  6]\n",
      " [-3  2 -1  2  5 -5  0 -7 -5  5  1 -3 -6 -1 -4  5 -5  1  1 -4 -3  6  4  5\n",
      "  -6  4 -1 -3 -3 -2  8 -2]\n",
      " [-8  3  5 -3  5  1 -3  7 -2  4  5 -4 -5 -3 -8  6  7  5  6 -3 -3 -7 -6 -6\n",
      "  -4  0  2  3  7 -4  3  7]\n",
      " [ 4  0 -2  4  6  2 -6  6 -6 -5  6 -5  0 -4 -2  4  2  2  4 -2  7  5  5  5\n",
      "   3 -1  5  7  3  4  2 -5]\n",
      " [ 2  6 -3 -7  3  6 -7 -3  1  2 -6  5 -2  1 -8  6  3  0  0  4  5  2  5  6\n",
      "   4 -6 -5  3  4  4 -8 -4]\n",
      " [ 4 -1  1  4  0 -6  3  6  6  0 -2  7 -3  4 -7  8 -1 -4  5 -4  2  3 -5  5\n",
      "  -7  8  5 -6  2  5 -6  0]\n",
      " [ 5  4 -4  1  0 -2 -3 -5  3 -7  7  6  1 -1  4  0 -3 -3 -1 -4 -1 -6 -6  1\n",
      "  -3  3 -6  4  1  7  1  4]\n",
      " [-1  3 -7  1  4  1  8  6  7 -5  1  6  3  1 -7  2 -6 -7  5  2  5  1 -5 -7\n",
      "  -1  6  2 -4 -2  1  8  0]]\n",
      "sequential_1/quant_dense_5/MatMul;sequential_1/quant_dense_5/BiasAdd\n",
      "[1 9]\n",
      "(0.0470588244497776, 0)\n",
      "[[116 105 122 101  47  70  97 107 101]]\n",
      "sequential_1/quant_dense_5/Softmax\n",
      "[1 9]\n",
      "(0.00390625, -128)\n",
      "[[100 101 114  42  11  10   5 115 104]]\n",
      "Identity\n",
      "[1 9]\n",
      "(0.0, 0)\n",
      "[[0.00 9816562535873973846016.00 70975523162658881667072.00 0.00 0.00\n",
      "  1136184828606280663178428809216.00 75550808434831052443272414756864.00\n",
      "  17033010714412843008.00 305737475180231515121659477491712.00]]\n"
     ]
    }
   ],
   "source": [
    "interpreter.allocate_tensors()\n",
    "for layer in interpreter.get_tensor_details():\n",
    "    # to store layer's metadata in group's metadata\n",
    "    print(layer['name'])\n",
    "    print(layer['shape'])\n",
    "    # grp.attrs[\"dtype\"] = all_layers_details[i]['dtype']\n",
    "    print(layer['quantization'])\n",
    "\n",
    "    try:\n",
    "        x = interpreter.get_tensor(layer['index'])\n",
    "        print(np.array2string(x, formatter={'float_kind':lambda x: \"%.2f\" % x}))\n",
    "    except ValueError:\n",
    "           pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
